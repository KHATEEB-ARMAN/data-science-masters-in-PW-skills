{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbe1606-8b57-4937-a05e-d768bbb8cf28",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39bcafd0-27a4-4f27-a59f-c6ce3170e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity \n",
    "# to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98659585-937a-4ca6-9652-581ab76cde03",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6dc96dc-7e8b-4afd-a454-2d1bdd58af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find \n",
    "# the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c1b4d-7cf1-480e-b609-37d71af47f96",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0888c44-147e-40ba-83b6-caf18f8dfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors.\n",
    "# Knn Regression: Predicts a value by using the mean of the k nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e1f56-fb47-4fca-b52c-ca37719bbbb7",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0951dd5-5099-40e3-9763-58ca5965846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output.\n",
    "# The traditional k-NN classifier works naturally with numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e50d5b-07c5-424d-9715-4e4b2f18047f",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56551c0-b004-4558-9fcb-e080914a4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the phenomenon where the feature space\n",
    "# becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac232ca7-ffe6-4892-9d70-e7ea27cec033",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a619697f-9b1e-49f3-b5ca-42a13f7afcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the\n",
    "# value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e24e0e-4e9d-4a04-a5d3-2f81fa2ebbeb",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2dff6f-148c-4499-8822-95173ab68d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. KNN regression tries to predict the value of the output variable by using a local average.\n",
    "# 2. Knn Regression: Predicts a value by using the mean of the k nearest neighbors.\n",
    "# 3. regression model: codomain of model is a continuous space, e.g. R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1cc0e5c-e6f7-4b33-a5cb-c7047e961e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.KNN classification attempts to predict the class to which the output variable belong by computing the local probability.t\n",
    "# 2.Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors.\n",
    "# 3.classification model: codomain of model is a discrete space, e.g. {0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e432a-6f7b-43fb-b629-75851a5ca4df",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63af4f6a-701e-4d51-90a2-6c4326544442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key differences are: KNN regression tries to predict the value of the output variable by using a local average.\n",
    "# KNN classification attempts to predict the class to which the output variable belong by computing the local probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf405850-4c53-4cf1-9f5d-f64415b7f99b",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f5a9004-7115-42cd-96fe-9dffaf997df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formula to calculate Euclidean distance is: For each dimension, we subtract one point's value from the other's to get the length of that “side” of \n",
    "# the triangle in that dimension, square it, and add it to our running total. The square root of that running total is our Euclidean distance.\n",
    "\n",
    "# for euclidient distance p=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a076cbb-f97c-46cd-a4a8-2bc958e6acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data. Hamming distance is used to\n",
    "# measure the distance between categorical variables, and the Cosine distance metric is mainly used to find the amount of similarity between two data points.\n",
    "\n",
    "# for manhatten distance p=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5847a46-3ed5-44e8-aa19-957eb197be4e",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c7207ed-8e87-4181-b403-ac83c2c23130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KNN algorithm does not work well with large datasets. The cost of calculating the distance between the new point and each existing point is huge, \n",
    "# which degrades performance. Feature scaling (standardization and normalization) is required before applying the KNN algorithm to any dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
