{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842f265b-fb6d-44d1-a960-eb5723ef0bfe",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf92ab9-2f66-4590-8b1b-8c56766f0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random forest regressor. A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f7a83-1aee-4599-91f9-5f7ceea18a54",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfef2902-c683-4463-a2b1-5a51ef319fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One straightforward solution to overfitting in random forests is to reduce the number of trees. You can experiment with different numbers of trees and choose the one that gives the best performance on the test data. Generally, a smaller number of trees can help reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241a0ed-66e4-4231-8b6a-6070b90428b1",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63321456-9cd5-4b9f-bee6-29fa28f0a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest (RF) models are machine learning models that make output predictions by combining outcomes from a sequence of regression decision trees. Each tree is constructed independently and depends on a random vector sampled from the input data, with all the trees in the forest having the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45904a0f-9af3-4644-be2c-5823ae424d11",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312d052a-9c22-41bb-bff0-7e0f7d39d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NÂº of Decision Trees in the forest (in Scikit-learn this parameter is called n_estimators)\n",
    "# The criteria with which to split on each node (Gini or Entropy for a classification task, or the MSE or MAE for regression)\n",
    "# The maximum depth of the individual trees. The larger an individual tree, the more chance it has of overfitting the training data, however, as in Random Forests we have many individual trees, this is not such a big problem.\n",
    "# The minimum samples to split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e91159-e70e-467a-bfe4-17d187be3784",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073af799-3b40-4ed8-8afd-436e2bda0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest reggresor is a ensemble learning method, which means it uses a combination of multiple models to make predictions.\n",
    "#  the result is taken from average of all sub trees.\n",
    "# decision tree is a single model that makes predictions based on a series of if-then rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f78848-cf10-4992-942c-8bf53c4b913d",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebeaf161-add5-4137-bd8c-6c2fc45af21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantage:\n",
    "\n",
    "# It can perform both regression and classification tasks. A random forest produces good predictions that can be understood easily.\n",
    "# It can handle large datasets efficiently. The random forest algorithm provides a higher level of accuracy in predicting outcomes over the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f6d7db-20f9-4e15-affb-f7a8f70de0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disadvantage:\n",
    "\n",
    "# A more accurate prediction requires more trees, which results in a slower model. In most real-world applications, the random forest algorithm is fast enough but\n",
    "# there can certainly be situations where run-time performance is important and other approaches would be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca9c2c-f873-4b4f-a738-2c2698679777",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1202ae82-180e-4fc6-b26f-eb978bb50810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each decision tree regression predicts a number as an output for a given input. Random forest regression takes the average of those predictions as its 'final' output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e15146-6cc4-4f91-bbd7-2dcd906a3866",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351b5273-54f7-4d08-8ea2-483d928943e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems.\n",
    "# It builds decision trees on different samples and takes their majority vote for classification and average in case of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77452bf-19d0-4326-859c-ae82430468d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
