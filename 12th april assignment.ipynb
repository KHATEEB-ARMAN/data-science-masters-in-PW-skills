{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888be7d8-2bb4-4c7e-9083-615a6f705950",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f167fb-f49e-47ca-8bd8-be5e828f638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging attempts to reduce the chance of overfitting complex models. It trains a large number of “strong” learners in parallel.\n",
    "# A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners together in order to “smooth out” their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baf694-7cc8-42f2-9134-2ec684a6e16d",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6448c24-300a-474f-a82f-7e00d9db26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantage:\n",
    "\n",
    "# Reduction of variance: Bagging can reduce the variance within a learning algorithm. This is particularly helpful with high-dimensional data, where missing values\n",
    "# can lead to higher variance, making it more prone to overfitting and preventing accurate generalization to new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e4ed47-8121-42ec-9252-39ca3e7a10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disadvantage:\n",
    "\n",
    "# Underfitting: Sometimes, it can result in underfitting if they have not properly trained the model.\n",
    "# Costly: Expensive in terms of using several models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043b9f2-6300-40ff-b725-17d8a9c0520d",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55087721-8faf-477f-a49d-917ae1554df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging performs well on high variance models like decision trees. On lower variance models such as linear regression, it is not expected to affect the \n",
    "# learning process. However, as per an experiment documented in this article, the accuracy reduces when bagging is carried out on models with high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9196793-a4fd-4aaf-ac6d-91dfdfbcaeb5",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d2a7ed-b0ba-4e28-9b36-c5b3535317b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is that by combining the strengths of multiple models, we can create a model that is more robust and less likely to overfit the data. It can be used\n",
    "# for both classifications and regression tasks. Ensemble learning techniques can be categorized in three ways: Bagging (Bootstrap Aggregating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92920986-5904-4fa8-9917-e18a22172352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging avoids overfitting of data and is used for\n",
    "# both regression and classification models, specifically for decision tree algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802455a-96c9-410e-b5b1-7b0d3195e3d8",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0247f-2e8f-46ca-aaea-f8490b98d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ensemble size is always 10 and the first ensemble members are trained using 95% of the data sampled without replacement. So the estimators are all very\n",
    "# similar to each other. Then the next ensemble is trained using 90% of the data and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608166b-d3e4-44ff-b8b8-55824a6875af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most popular ensemble methods are boosting, bagging, and stacking. Ensemble methods are ideal for regression and classification, where they reduce bias\n",
    "# and variance to boost the accuracy of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc8d87-a59f-4301-85eb-b073380d22d5",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880fe6e-f25d-440a-86e3-660fcce92ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bagging technique is used across a large number of industries, providing insights for both real-world value and interesting perspectives,\n",
    "# such as in the GRAMMY Debates with Watson. Key use cases include: Healthcare: Bagging has been used to form medical data predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
