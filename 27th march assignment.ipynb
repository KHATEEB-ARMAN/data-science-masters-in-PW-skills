{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823ff9d5-fd3e-4a73-96c7-863ea51fe0a4",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef255d92-f304-4bd9-9b71-4bcdd206c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion\n",
    "# of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well\n",
    "# the data fit the regression model (the goodness of fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4eecf3-3ada-4c3f-841f-3499c47cef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d07d77-ef41-4c18-ba36-2e6209684d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t can be interpreted as the proportion of variance of the outcome Y explained by the linear regression model.\n",
    "# It is a number between 0 and 1 (0 ≤ R2 ≤ 1). The closer its value is to 1, the more variability the model explains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe15c2-9d35-450d-9c6c-281989bee435",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2eae6b-8fae-4e73-a3fa-4e4fa5d862ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
    "# The adjusted R-squared increases when the new term improves the model more than would be expected by chance.\n",
    "# It decreases when a predictor improves the model by less than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1f41f-c2db-4bac-8b4b-2b70917b3a43",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530209e-9a95-4414-a1f6-be8c4de5747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model.\n",
    "# This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cdc53-f4bb-4656-8878-0c070312c41c",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb75fc-8f23-4640-b13c-8a8d07720109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually the metrics used are the Mean Average Error (MAE), the Mean Squared Error (MSE) or the Root Mean Squared Error (RMSE).\n",
    "# In short, MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a regression,\n",
    "# taking the average over all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ae8bf4-d47e-4f7e-8348-cc8fc6f84170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE=1nn∑i=1|yi-^yi|\n",
    "# MSE=1nn∑i=1(yi-^yi)2\n",
    "# RMSE=√1nn∑i=1(yi-^yi)2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8ed26-4ddf-41f7-888a-0e37fcfa5bef",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9341cd2-48b0-4a4a-aabb-0e42f989580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example,\n",
    "# if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5,\n",
    "# then MAE is more appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f8920-0a5c-4e6c-a0fb-5cfc26f68dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases.\n",
    "# One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly.\n",
    "# RMSE increases with an increase in the size of the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e62cd-fa36-4d5c-b47a-e0ea62dc9ba0",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0085236-f480-45f7-9ca4-6c56c09dc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in linear regression models\n",
    "# by including a penalty term in the cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4a423-62ec-4e3c-aa0f-8207057eb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  In contrast to Ridge regression, it adds the total of the absolute values of the coefficients rather than the sum of the squared\n",
    "# coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e70023a-29d0-4fb5-8203-42b5b4f94db8",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab85e9-b50b-4c49-88d8-c9eca79f7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated.\n",
    "# Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function\n",
    "# of the linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b6213-7fc9-4c91-9d30-c92680d0249c",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37a65a-801f-4172-8b02-330de4de5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Non-Linearity of the response-predictor relationships.\n",
    "# 2. Correlation of error terms.\n",
    "# 3. A non-constant variance of the error term [Heteroscedasticity]\n",
    "# 4. Collinearity.\n",
    "# 5. Outliers and High Leverage Points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0b603-84f1-4240-bc7a-a6ccf7e708a4",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ba7f0-2b72-47e9-a873-df545e833edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model B is Better.\n",
    "# because high RMSE is BAD performer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
