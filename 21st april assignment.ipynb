{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1590727f-5a80-47c9-ba1d-d4c88007752f",
   "metadata": {},
   "source": [
    "## .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01344608-b19d-4d2d-b515-82f172955c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Euclidean distance metric corresponds to the L2-norm of a difference between vectors and vector spaces. The cosine similarity is proportional to \n",
    "# the dot product of two vectors and inversely proportional to the product of their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1a0e71-2010-4109-b638-c8993b805cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. \n",
    "# In a simple way of saying it is the total sum of the difference between the x-coordinates and y-coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9137345-5a1d-4a0d-aae2-e7d471e0b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n addition, the performance of the KNN with this top performing distance degraded only about 20% while the noise level reaches 90%, \n",
    "# this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerate noise to a certain degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875ce93-5c74-463f-92a6-f96e41867a08",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e838114-b10d-4da9-a6cd-f9b02665e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most \n",
    "# favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e75cb6-4054-4555-977a-ebc453785ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer\n",
    "# elements in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510c98d-f469-4192-b3e9-906fa9de7882",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0a7c8c-ee74-4c92-880a-eaf18f2d6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN algorithm uses distance metric techniques to identify the similarities and dissimilarities between the data points. The similarity between the data\n",
    "# points increases with a decrease in the distance between them. The kNN algorithm stores the training data and then uses these data instances to predict the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f797f81-2d99-4bf7-9baa-536add3ff9e1",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec07871-a9a5-48e7-8d44-118030ccfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The k in kNN or K-Nearest Neighbour algorithm.\n",
    "# Learning rate for training a neural network.\n",
    "# Train-test split ratio.\n",
    "# Batch Size.\n",
    "# Number of Epochs.\n",
    "# Branches in Decision Tree.\n",
    "# Number of clusters in Clustering Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62bc6e04-811b-4ae4-a233-278763c47c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It involves splitting the dataset into training and testing sets and evaluating the model's performance on the testing set. 6. Grid search: Grid\n",
    "# search is a hyperparameter tuning technique that involves testing a range of values for each hyperparameter to find the best combination of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00d6ff-e570-4ad5-9eef-29c3567642a0",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d7f171b-94c7-4718-9874-25b1a3218c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have more training data, the “nearest neighbor” is probably closer (meaning the input is even more similar). More similar input means,\n",
    "# in theory, more similar output. KNN is a local approximation algorithm, and the more training data we have the more our local approximation is caracterised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c19e45fa-12ff-4ba0-a652-f0f20b4e0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KNN algorithm does not work well with large datasets. The cost of calculating the distance between the new point and each existing point is huge,\n",
    "# which degrades performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd1cbc-ccc7-483a-a4e3-e63a754b7191",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ce72e8-0e9e-4ac4-b85a-cbc188af30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's main disadvantages are that it is quite computationally inefficient and its difficult to pick the “correct” value of K. However, the advantages of this\n",
    "# algorithm is that it is versatile to different calculations of proximity, it's very intuitive and that it's a memory based approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b3210e-544e-42e9-8f5d-935ffa5c3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced Data – The KNN algorithm can struggle with imbalanced data, where one class has significantly more data points than the other.\n",
    "# This can lead to biased classification results, where the majority class is always predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c080a2-409a-447d-87c0-e72b8fae6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN is sensitive to outliers and missing values and hence we first need to impute the missing values and get rid of the outliers before applying the KNN algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
