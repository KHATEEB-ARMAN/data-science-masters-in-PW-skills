{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6df16ba-8801-4525-b751-ce81dc8bdd31",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d8754-4216-4499-b2e0-84c495826b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e02e1-8512-47c9-ac99-190379ca0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased,\n",
    "# and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef7d4d-173f-4777-8ddb-77feee0d3174",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a2f47-08c6-496a-9014-5fd5e873d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assumptions are the same as those used in regular multiple regression: linearity, constant variance (no outliers),\n",
    "# and independence. Since ridge regression does not provide confidence limits, normality need not be assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8bcabe-9977-47ab-9230-b4ba84e8fc2b",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7777f6-1843-441c-a12d-6988cf0f0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We iterate certain values onto the lambda and evaluate the model with a measurement such as 'Mean Square Error (MSE)'.\n",
    "# So, the lambda value that minimizes MSE should be selected as the final model. This ridge regression model is generally\n",
    "# better than the OLS model in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a885e26-a8bd-481b-8d7e-c5ac7ba0ed64",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47122ae1-1bf9-4a24-bd5b-1634d0083576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In one of our articles, we have seen that ridge regression is used to get rid of overfitting which can also be reduced by\n",
    "# fitting the model with only important features. Ridge regression can also help us in feature selection to find out the important\n",
    "# features required for modelling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc72ce6-5164-4e59-8ea9-2e41ef3e77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Stepwise Regression.\n",
    "# 2. Forward Selection.\n",
    "# 3. Backward Elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6dfb9-4512-486c-8f54-147c77fd390c",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c6f65-e2db-41d6-a4db-29a40d598ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression aims at reducing the standard error by adding some bias in the estimates of the regression.\n",
    "# The reduction of the standard error in regression estimates significantly increases the reliability of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c243c9-99f3-495f-93ab-142a45418bba",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12823c53-d192-49af-b5c8-545514bd66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within SPSS there are two general commands that you can use for analyzing data with a continuous dependent variable and one\n",
    "# or more categorical predictors, the regression command and the glm command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8647435-77a9-40e0-944f-8654acccd70a",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c10693-ba58-4685-aac4-d4df5fa6bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but\n",
    "# very small values. The lasso coefficients become zero in a certain range and are reduced by a constant factor, which explains their\n",
    "# low magnitude in comparison to the ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d63a0-862f-4e69-a7c8-ae1dbb491fec",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7d0a1-39ad-46dd-91ea-34e9f45cc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ridge regression technique can be used to predict time-series. Ridge regression (RR) can also solve the multicollinearity \n",
    "# problem that exists in linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
