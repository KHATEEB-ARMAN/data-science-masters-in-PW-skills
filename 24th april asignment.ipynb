{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd6ae41-84c5-49e3-b6dd-792a538bce87",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81f37a7-7738-4783-a147-4ad708795ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal component analysis or PCA, in essence, is a linear projection operator. that maps a variable of interest to a \n",
    "# new coordinate frame where the axes. represent maximal variability. Expressed mathematically, PCA transforms an. input data\n",
    "# matrix X (N × D, N being the number of points, D being the."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbee577c-2b1b-450a-8e3a-a5b761463a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, what we really want is the projection of the original data onto the new dimension. The last step of PCA is we\n",
    "# need to multiply Q tranpose of Q with the original data matrix in order to get the projection matrix. We go from the (d x k)\n",
    "# Q matrix and Q transpose of Q results in d x d dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ff63d-38fa-4e10-87f6-f02154d71409",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c12473-abe0-4363-98a1-9d1d9f4a21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA can be used to reduce the dimensionality of the data by creating a set of derived variables that are linear combinations\n",
    "# of the original variables. The values of the derived variables are given in the columns of the scores matrix Z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3675ec4c-0628-41e8-bceb-5656cd62e541",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fedcb08-bd7a-46d4-994b-f82e77bacbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: The eigenvectors\n",
    "# (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6024d-e075-4241-827f-db0974fe24a2",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c8e5fa-0867-4296-8c73-0ed89a9fdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The common way of selecting the Principal Components to be used is to set a threshold of explained variance, such as 80%,\n",
    "# and then select the number of components that generate a cumulative sum of explained variance as close as possible of that threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c760d-f25b-4ce5-8cb2-a9e7c39c10e3",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6481b0fb-1fc1-4ae9-b963-6b15837c1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply PCA to your data, you need to standardize each variable to have zero mean and unit variance. This ensures\n",
    "# that each variable contributes equally to the PCs and avoids bias. Then, compute the covariance matrix which measures\n",
    "# how each variable varies with every other variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8f5ac6-5230-4ef4-a41f-08fa9ae00f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes Correlated Features\n",
    "# Improves Algorithm Performance\n",
    "# Reduces Overfitting\n",
    "# Improves Visualization\n",
    "# Independent variables become less interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bc364-aa82-4af9-9ca9-da7b4af912f8",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0852b6bc-fd9b-427d-b125-177a96ab0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is used to visualize multidimensional data.\n",
    "# It is used to reduce the number of dimensions in healthcare data.\n",
    "# PCA can help resize an image.\n",
    "# It can be used in finance to analyze stock data and forecast returns.\n",
    "# PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f4816-845a-41a0-a3e0-581a02aca508",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76a64b9-a466-4ed5-bc06-46984b13d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variance explained can be understood as the ratio of the vertical spread of the regression line (i.e., from the lowest point\n",
    "# on the line to the highest point on the line) to the vertical spread of the data (i.e., from the lowest data point to the highest data point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709961e-0914-46f3-9cba-5a25b54ba700",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3242fc9-8f4e-4124-8083-66a01ccab900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA works by finding the directions of maximum variance in the data set and projecting the data onto these directions.\n",
    "# The principal components are ordered by the amount of variance they explain and are used for feature selection, data compression,\n",
    "# clustering, and classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c564785-caa0-4367-948a-be99f0256b4b",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba8ee07-f377-4600-8f19-8b63a15095c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions\n",
    "# of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
