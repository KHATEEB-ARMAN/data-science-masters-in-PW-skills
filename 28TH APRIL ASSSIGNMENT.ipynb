{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9829c129-e692-4903-be22-283b434bdd1c",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb31b5e5-9b60-4a8f-b697-380fb931979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering is a connectivity-based clustering model that groups the data points together that are close to each other\n",
    "# based on the measure of similarity or distance. The assumption is that data points that are close to each other are more similar or \n",
    "# related than data points that are farther apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6618ec89-a1c0-41c9-8e57-3d5b4d82ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. K- means clustering a simply a division of the set of data objects into non-overlapping subsets (clusters) such that each data\n",
    "#      object is in exactly one subset).\n",
    "# 2.A hierarchical clustering is a set of nested clusters that are arranged as a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41a936-4a30-40e8-b57f-a6b3af0ccb7c",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f3ab60-d8a9-4848-aab1-a8f23550a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you go down the hierarchy from 1 cluster (contains all the data) to n clusters (each observation is its own cluster),\n",
    "# the clusters become more and more similar (almost always). There are two types of hierarchical clustering.\n",
    "\n",
    "# 1.divisive (top-down) and \n",
    "# 2.agglomerative (bottom-up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e106c043-0835-4dbc-b043-c4c8ead7d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisive:\n",
    "\n",
    "# Divisive hierarchical clustering works by starting with 1 cluster containing the entire data set. The observation with the highest\n",
    "# average dissimilarity (farthest from the cluster by some metric) is reassigned to its own cluster. Any observations in the old cluster\n",
    "# closer to the new cluster are assigned to the new cluster. This process repeats with the largest cluster until each observation is its own cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf984da-5944-405d-9938-2c7c095316b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative:\n",
    "\n",
    "# Agglomerative clustering starts with each observation as its own cluster. The two closest clusters are joined into one cluster. \n",
    "# The next closest clusters are grouped together and this process continues until there is only one cluster containing the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791ebf2-ae8d-4af3-b839-518686c719b3",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1108ff10-0e1b-4a7e-be4c-41158663bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between\n",
    "# two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the\n",
    "# arrow between their two closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "273870d7-aad3-4838-baa7-44fae01123b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The four types of distance metrics are \n",
    "# 1.Euclidean Distance. \n",
    "# 2.Manhattan Distance. \n",
    "# 3.Minkowski Distance and Hamming Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473ed78-0f00-4531-86e0-e053990b7c1f",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918e4cee-a6d8-4e97-85ab-9dfe14d4edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that shows\n",
    "# the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will join them in a graph and the height\n",
    "# of the join will be the distance between those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9cce77-fa44-4b95-8edd-384dfa91d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap Statistic\n",
    "# Elbow Method\n",
    "# Silhouette Coefficient\n",
    "# Calinski-Harabasz Index\n",
    "# Davies-Bouldin Index\n",
    "# Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cce8ef-7583-4b0a-8d2c-63f637629658",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67121c74-7a37-46a5-b9bd-f176269c3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation.\n",
    "# The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending \n",
    "# on the selected distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7a3049-ba08-486f-b21a-d865370b47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering groups similar objects into a dendrogram. It merges similar clusters iteratively, starting with each data\n",
    "# point as a separate cluster. This creates a tree-like structure that shows the relationships between clusters and their hierarchy.\n",
    "# We are essentially building a hierarchy of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb78c9-4e3e-402b-af8e-05b43c2b78ac",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96cf417e-b657-4d69-a432-29bea20b1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most distance metrics, and hence the hierarchical clustering methods, work either with continuous-only or categorical-only data.\n",
    "# In applications, however, observations are often described by a combination of both continuous and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb21a3-d75e-4696-a924-b3b970072d3a",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c56da49-bc9b-47f4-a87b-c706db82e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In hierarchical clustering, by using dendrogram outliers are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bae8bde-0763-4473-936b-39ae865033a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHC is capable of performing outlier detection, component population forming and updating, and clustering in the same step.\n",
    "# This is enabled by the statistical agglomeration concept, which allows outliers and components to be agglomerated based on the\n",
    "# statistical relations between them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
