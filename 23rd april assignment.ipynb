{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a83301-8e5d-471d-bc61-041770e39e1f",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b1407c-09b3-422d-a5cb-6d0a034ea6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curse of dimensionality is a common problem in machine learning, where the performance of the model deteriorates as the number\n",
    "# of features increases.\n",
    "# This is because the complexity of the model increases with the number of features, and it becomes more difficult to find a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aeb1191-2429-4ae6-863b-d753c62fa28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction can help to mitigate these problems by reducing the complexity of the model and improving its generalization performance.\n",
    "# There are two main approaches to dimensionality reduction:\n",
    "# 1. feature selection 2. feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75639ece-8f5c-488a-8428-e63484e754c9",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1411707-6dab-4dc5-ae3b-6f5bf24fedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the dimensionality increases, the number of data points required for good performance of any machine learning algorithm increases exponentially.\n",
    "# The reason is that, we would need more number of data points for any given combination of features, for any machine learning model to be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72365432-e206-43ab-a7a7-a7e4c4d21ea0",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a41949-c234-47c6-ada2-acdccff2e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the number of dimensions or features increases, the amount of data needed to generalize the machine learning model accurately increases exponentially. \n",
    "# The increase in dimensions makes the data sparse, and it increases the difficulty of generalizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ef42c2-f08a-48a3-b2ba-3cd19de581d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason is that, we would need more number of data points for any given combination of features, for any machine learning model to be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329eb37-9507-4dd9-afa8-9ccf185161c9",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2bc474-959b-4b00-a44a-6bf17274d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection:\n",
    "# Feature selection involves selecting a subset of the original features that are most relevant to the problem at hand. The goal is to reduce the\n",
    "# dimensionality of the dataset while retaining the most important features. There are several methods for feature selection, including filter methods, wrapper methods\n",
    "# and embedded methods. Filter methods rank the features based on their relevance to the target variable, wrapper methods use the model performance as the criteria for\n",
    "# selecting features, and embedded methods combine feature selection with the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b980a4f7-4cc6-4b26-b5e1-ff013554e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: In this, we try to find a subset of the original set of variables, or features, to get a smaller subset which can be used to model the problem. It usually involves three ways:\n",
    "# 1.Filter\n",
    "# 2.Wrapper\n",
    "# 3.Embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e5976-d093-487f-9994-90e4ba214074",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb901f6-f2e7-436d-bd67-45c017a5b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
    "# 2.It may need a lot of processing power.\n",
    "# 3.Interpreting transformed characteristics might be challenging.\n",
    "# 4.The independent variables become harder to comprehend as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594d2aa-4bd1-4cb7-9373-b0641e05714d",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dcf314b-9f22-44f4-9b2b-35c1374134b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over fiting:\n",
    "\n",
    "# Data sparsity is one of the facets of the curse of dimensionality. Training a model with sparse data could lead to high-variance or overfitting\n",
    "# conditions. This is because while training the model, the model has learnt from the frequently occurring combinations of the attributes and can\n",
    "# predict the outcome accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fe389b-c816-4847-826c-55a65d86e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the phenomenon where the feature \n",
    "# space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10d54b-1946-4942-9f91-ba9232f10b0f",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388927e1-5cd4-4d82-9175-5bed5d3b0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible.\n",
    "#   This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data.\n",
    "# 2.Techniques for dimensionality reduction include: principal component analysis (PCA), singular value decomposition (SVD), and linear \n",
    "#   discriminant analysis (LDA).\n",
    "# 3.Each technique projects the data onto a lower-dimensional space while preserving important information.\n",
    "# 4.Dimensionality reduction is performed during pre-processing stage before building a model to improve the performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
