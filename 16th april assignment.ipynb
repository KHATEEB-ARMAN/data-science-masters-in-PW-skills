{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe702dd5-e925-453b-823c-395815c19982",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4047a5-691f-47b3-a593-6891fbdb2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software,\n",
    "# called machine learning models, on labeled data to make guesses about unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bd122-fd65-4f86-9619-8a4ac9b9ad03",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c347474-ea72-43f2-9377-361b188fb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting is more suitable for data with low variance, high bias, and high noise, as it can reduce underfitting and increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cc69c-e002-46fe-a464-b86484448f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation:\n",
    "\n",
    "# Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier\n",
    "# is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807b771-848b-47b4-91f5-0669b624fb06",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157245a9-7949-491b-ac18-29659b4f86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. \n",
    "Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a941a01-b5dd-42e1-9cfb-7bdf62c0d554",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20c6bb6-d0f5-434f-81ff-42e03637c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost. \n",
    "# Gradient tree boosting.\n",
    "# eXtreme Gradient Boosting - XGBoost.\n",
    "# LightGBM. \n",
    "# CatBoost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc4ed8-eb14-425e-9af9-acbb8ebfd4f9",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71a5fc-ed91-4585-8fd0-df12d5079d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate: This determines the impact of each tree on the final outcome (step 2.4). ...\n",
    "# n_estimators: The number of sequential trees to be modeled (step 2) ...\n",
    "# subsample: The fraction of observations to be selected for each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add7afe-bbb3-4c75-86db-58dbbe1238cc",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822f298-6e14-4948-9b35-118168644f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff198795-e99f-43c6-a13b-89142ec4b507",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b33316-9a43-4071-9ce4-8b9df1105e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost is an ensemble learning technique used to improve the predictive accuracy of any given model by combining multiple “weak” learners. Adaboost works by weighting incorrectly classified instances more heavily so that the subsequent weak learners focus more on the difficult cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51de28-8569-47e0-b6f9-be8d8833728b",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca169575-d5e5-4a97-ac0b-4e06a4eeb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error function that AdaBoost uses is an exponential loss function. First we find the products between the true values of training samples and the overall prediction for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24546e7a-8dbc-471b-9553-2188d8394c51",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53194a85-cead-4fa1-877f-ae1f470de052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost works by weighting incorrectly classified instances more heavily so that the subsequent weak learners focus more on the difficult cases. It is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cef443-3c32-48b2-b711-78d46b9d0a83",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43848e56-dace-48ab-898a-a4acf8478fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate depends highly upon the number of n_estimators. By default, it is set to 1 but it can be increased or decreased depending on the estimators used. Generally, for a large number of n_estimators, we use a smaller value of learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af27b5-253f-4729-a409-c0630a638249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
